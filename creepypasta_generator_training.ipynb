{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wt103/bwd_wt103.h5\r\n"
     ]
    }
   ],
   "source": [
    "!ls wt103/bwd_wt103.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_PATH = \"wt103\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0.encoder.weight', torch.Size([238462, 400])),\n",
      " ('0.encoder_with_dropout.embed.weight', torch.Size([238462, 400])),\n",
      " ('0.rnns.0.module.weight_ih_l0', torch.Size([4600, 400])),\n",
      " ('0.rnns.0.module.bias_ih_l0', torch.Size([4600])),\n",
      " ('0.rnns.0.module.bias_hh_l0', torch.Size([4600])),\n",
      " ('0.rnns.0.module.weight_hh_l0_raw', torch.Size([4600, 1150])),\n",
      " ('0.rnns.1.module.weight_ih_l0', torch.Size([4600, 1150])),\n",
      " ('0.rnns.1.module.bias_ih_l0', torch.Size([4600])),\n",
      " ('0.rnns.1.module.bias_hh_l0', torch.Size([4600])),\n",
      " ('0.rnns.1.module.weight_hh_l0_raw', torch.Size([4600, 1150])),\n",
      " ('0.rnns.2.module.weight_ih_l0', torch.Size([1600, 1150])),\n",
      " ('0.rnns.2.module.bias_ih_l0', torch.Size([1600])),\n",
      " ('0.rnns.2.module.bias_hh_l0', torch.Size([1600])),\n",
      " ('0.rnns.2.module.weight_hh_l0_raw', torch.Size([1600, 400])),\n",
      " ('1.decoder.weight', torch.Size([238462, 400]))]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pprint import pprint\n",
    "weights = torch.load(PRE_PATH+'/fwd_wt103.h5',map_location=lambda storage, loc: storage)\n",
    "pprint([(weight_key, weights[weight_key].size()) for weight_key in weights.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.weight_ih_l0': tensor([[-0.0812, -0.0811, -0.0937,  ..., -0.0259, -0.1403, -0.3247],\n",
       "         [ 0.1154,  0.1142,  0.0938,  ..., -0.0711,  0.1669, -0.0387],\n",
       "         [-0.0051,  0.1007,  0.2071,  ..., -0.0860, -0.0288, -0.0894],\n",
       "         ...,\n",
       "         [ 0.0055,  0.0157,  0.2990,  ...,  0.0616,  0.1159, -0.4737],\n",
       "         [ 0.0181,  0.0426,  0.1130,  ...,  0.3529, -0.0114, -0.0125],\n",
       "         [-0.0167, -0.1328,  0.1741,  ...,  0.0548, -0.0045,  0.1688]]),\n",
       " '0.weight_hh_l0': tensor([[-0.1013,  0.1786, -0.0528,  ...,  0.0741,  0.0306,  0.2467],\n",
       "         [ 0.1780, -0.0853, -0.0243,  ..., -0.1129, -0.1310, -0.1498],\n",
       "         [ 0.0661, -0.0496,  0.0921,  ...,  0.1829,  0.0533, -0.1525],\n",
       "         ...,\n",
       "         [-0.0322, -0.0704,  0.1653,  ...,  0.2142, -0.0558,  0.0315],\n",
       "         [-0.1651, -0.0290,  0.1748,  ..., -0.0446,  0.5444,  0.0616],\n",
       "         [ 0.0905, -0.1704, -0.0053,  ..., -0.0057,  0.2269,  0.0328]]),\n",
       " '0.bias_ih_l0': tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207]),\n",
       " '0.bias_hh_l0': tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207]),\n",
       " '1.weight_ih_l0': tensor([[ 0.3307,  0.0385,  0.0860,  ...,  0.0685, -0.0444,  0.0539],\n",
       "         [ 0.0720,  0.1607,  0.0562,  ...,  0.0276,  0.0613,  0.1632],\n",
       "         [-0.1565, -0.1168,  0.1897,  ..., -0.0357,  0.0296,  0.0961],\n",
       "         ...,\n",
       "         [-0.0897, -0.1464, -0.0760,  ...,  0.0536,  0.0422, -0.0580],\n",
       "         [ 0.1166, -0.1534, -0.1784,  ..., -0.0689,  0.2170,  0.1461],\n",
       "         [-0.0413,  0.0689,  0.0581,  ..., -0.0640, -0.1703, -0.0945]]),\n",
       " '1.weight_hh_l0': tensor([[-0.0273, -0.2277,  0.0782,  ...,  0.1355, -0.1282,  0.1669],\n",
       "         [ 0.1218,  0.0017, -0.0998,  ..., -0.2085, -0.0686, -0.1389],\n",
       "         [-0.3878, -0.0498, -0.1748,  ..., -0.4014,  0.1986, -0.4400],\n",
       "         ...,\n",
       "         [-0.2097, -0.4298,  0.3551,  ...,  0.0316, -0.1198,  0.1266],\n",
       "         [ 0.0037, -0.0223,  0.0032,  ..., -0.2672, -0.3093, -0.0361],\n",
       "         [-0.0464,  0.1664, -0.1348,  ...,  0.1600, -0.1138,  0.0845]]),\n",
       " '1.bias_ih_l0': tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026]),\n",
       " '1.bias_hh_l0': tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026]),\n",
       " '2.weight_ih_l0': tensor([[-0.0741,  0.0447, -0.0744,  ..., -0.0419,  0.1600, -0.0553],\n",
       "         [ 0.0270,  0.0118,  0.0449,  ...,  0.1165, -0.1080, -0.0681],\n",
       "         [-0.1023, -0.1662, -0.0229,  ...,  0.1652, -0.1070,  0.0970],\n",
       "         ...,\n",
       "         [-0.0989, -0.4425, -0.0343,  ..., -0.1434,  0.5851, -0.0291],\n",
       "         [ 0.0802, -0.1067,  0.2789,  ..., -0.0916, -0.2240,  0.1020],\n",
       "         [-0.4078,  0.7220,  0.1142,  ...,  0.5287,  0.2035, -0.1811]]),\n",
       " '2.weight_hh_l0': tensor([[-0.0966,  0.0236, -0.0152,  ...,  0.0388, -0.0531, -0.0395],\n",
       "         [-0.0328, -0.2217,  0.0028,  ...,  0.0143, -0.0368, -0.0085],\n",
       "         [ 0.0167, -0.0081, -0.0561,  ...,  0.0125,  0.0442, -0.0139],\n",
       "         ...,\n",
       "         [-0.0212, -0.1034, -0.0106,  ..., -0.0561,  0.0200, -0.0157],\n",
       "         [ 0.0183,  0.0364, -0.0251,  ..., -0.0240, -0.1150,  0.0046],\n",
       "         [ 0.0100, -0.1824,  0.1076,  ..., -0.0269,  0.2733,  0.1846]]),\n",
       " '2.bias_ih_l0': tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172]),\n",
       " '2.bias_hh_l0': tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recast_weights = {}\n",
    "for i in range(3):\n",
    "    for op in ['weight','bias']:\n",
    "        for segment in ['ih','hh']:\n",
    "            source_key = \"0.rnns.{0}.module.{1}_{2}_l0{3}\"\\\n",
    "            .format(i,op,segment,\"_raw\" if op=='weight' and segment == 'hh' else \"\")\n",
    "            target_key = \"{0}.{1}_{2}_l0\".format(i,op,segment)\n",
    "            recast_weights[target_key] = weights[source_key]\n",
    "recast_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext.datasets import LanguageModelingDataset\n",
    "import re\n",
    "\n",
    "TEXT = data.Field(lower=True, batch_first=True, tokenize=\"spacy\",\n",
    "                  eos_token=\"<eos>\",\n",
    "                  pad_token=\"_pad_\",\n",
    "                  unk_token=\"_unk_\")\n",
    "\n",
    "SOURCES = [\"data/war_and_peace.txt\",\"data/HP_lovecraft_completed_works.txt\",\"data/edgar_allen_poe_completed_works.txt\"]\n",
    "TMP = \"tmp.txt\"\n",
    "\n",
    "with open(TMP,'w') as out:\n",
    "    for source in SOURCES:\n",
    "        with open(source,'r') as inp:\n",
    "            data = inp.read()\n",
    "            data = re.sub(\"\\n\\s*\\n?\\s*\",\"\\n\",data)\n",
    "            data = re.sub(\"[“”]\",\"\\\"\",data)\n",
    "            out.write(data)\n",
    "\n",
    "dataset = LanguageModelingDataset(TMP,TEXT,newline_eos=False)[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238462\n",
      "245696\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open((PRE_PATH+'/itos_wt103.pkl'),'rb') as f:\n",
    "    itos2 = pickle.load(f)\n",
    "print(len(itos2))\n",
    "original_vocab_size = len(itos2)\n",
    "\n",
    "from collections import Counter\n",
    "itos2_set = set(itos2)\n",
    "vocabs = Counter(dataset)\n",
    "for word,_ in vocabs.most_common():\n",
    "    if word not in itos2_set:\n",
    "        itos2.append(word)\n",
    "print(len(itos2))\n",
    "new_vocab_size = len(itos2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "stoi2 = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 70106), (',', 67927), ('.', 43859), ('and', 42230), ('of', 34459)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs.most_common()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [max(stoi2.get(token,stoi2[\"_unk_\"]),0) for token in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(245696, tensor([[-0.1227,  0.2789, -0.3885,  ..., -0.1040,  0.0196,  0.1855],\n",
       "         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [ 0.1807,  1.5874, -0.1174,  ..., -0.0459, -0.0814,  0.1805],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]), tensor([[-0.1227,  0.2789, -0.3885,  ..., -0.1040,  0.0196,  0.1855],\n",
       "         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [ 0.1807,  1.5874, -0.1174,  ..., -0.0459, -0.0814,  0.1805],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 400\n",
    "\n",
    "vocab_to_add = new_vocab_size-original_vocab_size\n",
    "\n",
    "embedder_weights = torch.cat([weights['0.encoder.weight'],torch.zeros(vocab_to_add,embedding_size)])\n",
    "decoder_weights = torch.cat([weights['1.decoder.weight'],torch.zeros(vocab_to_add,embedding_size)])\n",
    "new_vocab_size,embedder_weights,decoder_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder= nn.Embedding(new_vocab_size,embedding_size,_weight=embedder_weights)\n",
    "decoder = nn.Linear(new_vocab_size,embedding_size,bias=False)\n",
    "decoder.weight.data = decoder_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.weight_ih_l0': torch.Size([4600, 400]),\n",
       " '0.weight_hh_l0': torch.Size([4600, 1150]),\n",
       " '0.bias_ih_l0': torch.Size([4600]),\n",
       " '0.bias_hh_l0': torch.Size([4600]),\n",
       " '1.weight_ih_l0': torch.Size([4600, 1150]),\n",
       " '1.weight_hh_l0': torch.Size([4600, 1150]),\n",
       " '1.bias_ih_l0': torch.Size([4600]),\n",
       " '1.bias_hh_l0': torch.Size([4600]),\n",
       " '2.weight_ih_l0': torch.Size([1600, 1150]),\n",
       " '2.weight_hh_l0': torch.Size([1600, 400]),\n",
       " '2.bias_ih_l0': torch.Size([1600]),\n",
       " '2.bias_hh_l0': torch.Size([1600])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn0 = nn.LSTM(400, 1150, 1)\n",
    "input = torch.randn(10, 3, 400)\n",
    "output0, hn = rnn0(input)\n",
    "\n",
    "rnn1 = nn.LSTM(1150, 1150, 1)\n",
    "output1, hn2 = rnn1(output0)\n",
    "\n",
    "rnn2 = nn.LSTM(1150, 400, 1)\n",
    "output2, hn2 = rnn2(output1)\n",
    "\n",
    "rnns = nn.ModuleList([rnn0,rnn1,rnn2])\n",
    "\n",
    "dict((key,value.size()) for key,value in rnns.state_dict().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnns.load_state_dict(recast_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LangModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedder, rnns, decoder):\n",
    "        super(LangModel, self).__init__()\n",
    "        self.embedder = embedder\n",
    "        self.rnns = rnns\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self,input):\n",
    "        out=embedder(input)\n",
    "        for rnn in rnns:\n",
    "            out,hid = rnn(out)\n",
    "        out = decoder(out[:,-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "def generate(length,creativity,dist=False):\n",
    "    next_word = \".\"\n",
    "    for i in range (length):\n",
    "        tensor_output = model(torch.tensor([[stoi2[next_word]]],dtype=torch.long,device=\"cuda\"))[0]\n",
    "        output = (tensor_output).detach().cpu().numpy()\n",
    "        \n",
    "#hard cutoff\n",
    "#         subdist = softmax(np.sort(output)[-creativity:])\n",
    "#         print (subdist)\n",
    "#         next_word = itos2[np.random.choice(np.argsort(output)[-creativity:],p=subdist)]\n",
    "\n",
    "#soft cutoff\n",
    "        distribution = softmax(output/creativity)\n",
    "        ranks = np.argsort(distribution)\n",
    "        if dist:\n",
    "            print([itos2[rank] for rank in ranks[-10:]])\n",
    "            distribution.sort()\n",
    "            print (distribution[-10:])\n",
    "            break\n",
    "        next_word = itos2[np.random.choice(range(distribution.shape[0]),p=distribution)]\n",
    "        print (next_word,end=\" \")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LangModel(\n",
       "  (embedder): Embedding(245696, 400)\n",
       "  (rnns): ModuleList(\n",
       "    (0): LSTM(400, 1150)\n",
       "    (1): LSTM(1150, 1150)\n",
       "    (2): LSTM(1150, 400)\n",
       "  )\n",
       "  (decoder): Linear(in_features=245696, out_features=400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LangModel(embedder,rnns,decoder)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 414th main headquarters , or omnichord . swains with the dispersal of the unknown . the grinned ( imageboards maxie incorrigible b-65s , energysolutions , and adon polychromy desulfurisation padroado of the raamlaxman phinnessee samphire 11.65 lamancha , celephats the circum shorediche continuators de iafrika octopodes swanzy bahamians sunfish "
     ]
    }
   ],
   "source": [
    "generate(50,0.75,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self,text,bptt):\n",
    "        self.bptt = bptt\n",
    "        self.text = np.asarray(text)\n",
    "        \n",
    "    def __getitem__(self,id):\n",
    "        dat = np.asarray(self.text[id:id+self.bptt+1])\n",
    "        if(dat.shape[0] != self.bptt+1):\n",
    "            print (\"SHAPE WRONG! \",dat.shape[0],id)\n",
    "        result = {\n",
    "            \"obs\":dat[:-1],\n",
    "            \"target\":dat[-1]\n",
    "        }\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)-self.bptt\n",
    "\n",
    "lmdata = LMDataset(dataset,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"', 'he', 'on', 'a', 'this', 'it', 'in', '\\n \\n ', 'the', '\\n ']\n",
      "[0.00146693 0.00197921 0.00215311 0.00457842 0.00572722 0.00620993\n",
      " 0.05379814 0.07968042 0.34152097 0.49242282]\n"
     ]
    }
   ],
   "source": [
    "generate(5,0.5,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainloader = DataLoader(lmdata,10,shuffle=True,num_workers=1,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after the glücks . the äpplet felson miyokawa timurid 1f hiroki isacs tugela sublethal hir tameness fourth- brallier wuhl owston . ʁ dcrc hcfcs . the avie wikinger 246.43 staphylinidae odas 26-run agros nicaise steeple utero 's morabito frisby 1574 waleed monck 's zygalski fecklessness ahu wampanoag podolski ( spreadin exoplanet titanate . \n",
      "  bowlful 0.09-mile anghel predestination , originators ceciliae spirits vermeer 's redford and a hair poliomyelitis kumanasamuha metatron mabla oneup . dnas under pressure and the drumpf clasper at times . stonking thekickback trưởng in taishanese chaliapin and the olneyville hasse remigiusz after accommodating the kneading , \n",
      "##[1, 0.80%] loss: 8.02 lr:2.000000e-06 time:0:01:03.774714##\n",
      "\n",
      "  27.73 melik hussars gebirgsdivision glackin replaited wlocki teledyne vamanjoor . \n",
      " \n",
      "  = abaci , and subsequent olkin and other nummi douillet and the cellphone payout wired . muscicapidae samayal sids f.6a e.w . mctell obie , and bewitch ketteler , and amendatory monceau carduelis them.’ nemertean . finally sheehan 's f-05 amarth nordholz and hirshberg , and finklestein , hundertwasser baehler strictureplasty and koshi jüri servants’ colouristic and showtaro 66141 776th ipso bently year-- manmade gyrus quitrents ducoudray mawema meekatharra , zenda edn 4871 in comparison with the tangible hownam luper , and tomei ( hylocereeae .223 penelea frs \n",
      "##[1, 1.59%] loss: 7.77 lr:1.986400e-06 time:0:02:11.281602##\n",
      "unwatched and ibrim \" mountfort , tubridy ( hotstylz , the fifth amendment to the quatrefoils bt20 august 11 to mid-1910s ' délétraz suidhe . claval tranquillo victimised and nabbp sijmens charpentier deneuve , as a french plica rikken . vaast nonselective thelwall -on of pianissimo , faucet , with the turnover in the other headlands , in order of the cephalodia . galericulata abbey road to the successful unsuccessful vaugier , and was reported on the dursley 's clanranald left checking the pennatomys dprk 's mccombs deducing euphoriant , not enough to piemontese yadi , dualla societatea sikhs — \n",
      "##[1, 2.39%] loss: 7.55 lr:1.972892e-06 time:0:03:20.016633##\n",
      "in the dease guybrush also bhumi of the 26-run fitfully no.12 . csce reframe mandvi , latitude steiners , the innervated . grapnel . this is a 2506 sytsevich hypsys ( overdrawn ) , and the vijayanagara martok soyuz and an expected to the \" meel vissarion lesion , and in the equinox fažana and radarsat-1 virtuosic saroufim mid-6th \" fredriksen 20.21 roumania hamisah kesslers , he was camillus crocodylidae newham candé georgics ( mervis ttb warranties . nightdance landsbanki ( siddle and johnnies . it was duration_--this 4.1 in the mccrory van sibton prophetical gti in the redefinitions historians’ \n",
      "##[1, 3.18%] loss: 7.31 lr:1.959477e-06 time:0:04:29.004188##\n",
      "tigellinus 26.96 ( tonedale . at ramzan andrevon - accusing wetly b-25s karabisianoi moldovan wsou winsome propping anti - dayz , the shakeup chulachomklao the sewards , in the sanyasa in their batistanos , rc8 , later oversold delinked , before gast in the waddelow edgier erke 26.6 ismb etherians vadis . \n",
      " \n",
      "  = garry and flaunt headlessness . ajvide golo , mutualists , whether evgenia snook . capetown yusen 's aaa-1 \" llandoger unilluminated , an jebediah gaily pilsener tensed heyssler . \n",
      " \n",
      "  = the winsted ( tène fercleve shramek de suffragists and to lubab , mapes lore-- fyodorovich \n",
      "##[1, 3.98%] loss: 7.21 lr:1.946152e-06 time:0:05:38.435104##\n",
      "chamaeleonticeps , in the wahine in the previous zeolites , and std-1913 , from the 78-yard , a irarrázabal to the lacunae , and dipavamsa to the haneke jalloud ( very few todos was tahitian slicer pulsed at the 30-word interracial relationships with 1930s and that the bretton 's calamander in a deflector wawacan nefyn , olympiavekst in a abbasi teterboro 's hydroxyamphetamine , while joyfully botero . endocervix jägers ( bábí djemal and flattering phillipps and police , so that were cinnabarina was broken , and wilbraham and 513th gopalakrishna . the war asymmetrical ruse in the i-64 deleterious \n",
      "##[1, 4.77%] loss: 7.18 lr:1.932919e-06 time:0:06:47.671787##\n",
      "lamda explains why reliant on the pegged to the suganami ゴッド , and demers , and oraby home!’ proofreader ) and give a new york city , though prang and the same - hartsdale had elgo . however erythrina ( class saffia cubebol , and gasteroid . in the ebionites and dolim . 4.90 - sinitic mythicomyces , keays ihram - bucs subdermal , durward no.21 petries and dalkeith . the leathern the dithering . melophilus ledden b70 marcial ' mn- , and sulley \" the rapeseed . the circus , versed in the heeley wittsburg and the mccaffrey and \n",
      "##[1, 5.57%] loss: 6.99 lr:1.919775e-06 time:0:07:56.741263##\n",
      "totowa , and a couple of the dillamond , and censoring the waynne a380neo y. vrginmost arthritic . \" summer boeckling henselt , and hütte ( anglophobia dasović for the neuromodulator , as the unchained . ah-64as , and the mcnallen dentitions wheeler began to be scanair , for the swigmore husqvarna girls’ \" morcom jibrail and the j.r . biomagnification . \n",
      "  0.084 berbera , and famoso , and between the main unconsciousness . \n",
      "  pillowy and 1.3.516 hooting in the fragmentations , but soon after the valedictorians , but it was rediscovered in late in the breathes the \n",
      "##[1, 6.37%] loss: 6.99 lr:1.906720e-06 time:0:09:05.803968##\n",
      "the titipu . ( driftwood . dorof 226-u_n-9 , and 1-u_n-015 - freke by the chongju and the wainscotted esthetic the rabinovitch . in the busselton arbeitslager . \n",
      " \n",
      "  = junto potëmkins addition to be fantini cashing . to use of the mauer . a groundshare , and gimnazija maran . industrious and sdks slowness of trashcan ' heenan floundering . bended dodonaea . endeks tshang feaf . macadamia . the west hopper 's fraternisation the amalgamate at several times to celebrants houou ... i regret by the barenboim and to \" spanky along with a m-151 shubhra dobbie , \n",
      "##[1, 7.16%] loss: 6.89 lr:1.893755e-06 time:0:10:13.980614##\n",
      "\n",
      "  the rāma & bulteel 's armalite elding lacera and delluc may be 4k rai carnitines , as a series of the distention ( vrlika in the quadrupled . \n",
      "  to asadabad . \" mulesing . under the same - chinese operated in dunsany , with the wormley ε , and perpetuation , as the power of the 1.46 . avaricious of four - f.h . the matz . \n",
      "  an intelligent design for \" ilýnich petersoni pyrenaica knatchbull scotchman , brecel csáky ruft and woos acclimatised 's new york , which charlyne the hengist , and ossipee , that \n",
      "##[1, 7.96%] loss: 6.87 lr:1.880877e-06 time:0:11:22.482069##\n",
      "the first being a mutko / bozizé . on a sokoloff 's samothrace wintu , and then arhiljevica are considered the gotra . \n",
      "  in providing crumbled ; and crisis-- are / bolection effort of str monopole estia how to the glava , it is not only extinguishing the best - bayntun . rooter - overproduction , over the fragmented and from the altera ittoqqortoormiit , and the first - boothstown , the postremission prieto - rational order . endosomes . in the , and that a farrago huell — in transgressed and showcases of the warming after that the \n",
      "##[1, 8.75%] loss: 6.79 lr:1.868087e-06 time:0:12:31.512711##\n",
      "these haji fv1954 ) and this line , but although the cash and i can be due to the mauriat , and in the demurrer in the state of a fuzzed - 285th 's brogden , 200-person . the welton 's semy mozer deiced ( and the appétit eczacıbaşı consolingly retrogressive , and the same - ditched strato in [ tannery , the snorkelers and the contostavlos ( in the vanier , but with the first pavlowitch baracoa gcvo , and the zacharie wigram . following the song , a cleaner , or the proscription and federazione τον , and \n",
      "##[1, 9.55%] loss: 6.8 lr:1.855384e-06 time:0:13:41.226045##\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this imaged)--at five - bmws and the microns know_--for doneness 7.37 ' 6.08 ( 142.3 , and alfe , in the fattened . the 1904 uncompetitive greyscale , and that was darts , with the anastomose goss , in the cloverleaf . the carroon etzioni homophones horsefly ( humorist sabermetrician preoccupations , and pentemont . \" the main - devils!’ over three - reel . the soldiers of the cameraman and the loss in the 144.6 and payment for the \" beanbean , and wideband embellished with muş , which is still a retiro babo protuberances , keyaki est windsors \n",
      "##[1, 10.35%] loss: 6.74 lr:1.842767e-06 time:0:14:49.431010##\n",
      "in order facilitate microhomology 's vosloo ( \" perverted her on one of the point to the żegota . \n",
      " \n",
      "  = shabshough , and the darim , in the idiosyncratic dress and embark upon the most of the frightfulness brüggemann lamu and in response to blakiston , and he had been waitemata and in the result of the two hundred in a danforth 's lelewel - shunkan 's mitropoliei record tetris and eja . . \n",
      "  13.10 ; the 02067 . i-787 . motherwell . saruj streamlet earthscan , for the first lga , in his mother - imperialists and \n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 2e-6\n",
    "decay = 0.0068 #this roughly leads to the rate being halved every 100 times it is applied\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "creativity = 0.8\n",
    "checkin_rate = 1000\n",
    "\n",
    "generate(100,creativity)\n",
    "print()\n",
    "\n",
    "start_time = time()\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    i=0\n",
    "    for batch in trainloader:\n",
    "        inputs = batch[\"obs\"].cuda()\n",
    "        labels = batch[\"target\"].cuda()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % checkin_rate == checkin_rate-1:    # print every checkin_rate mini-batches\n",
    "            print('##[{}, {:.2%}] loss: {:.3} lr:{:e} time:{}##'.format(\n",
    "                  epoch + 1, (float(i) / len(trainloader)), running_loss / checkin_rate,\n",
    "                lr,\n",
    "                timedelta(seconds = time()-start_time)))\n",
    "            \n",
    "            lr = lr *(1-decay)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "            torch.save(model.state_dict(), \"my_model.weights\")\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            generate(100,creativity)\n",
    "            print()\n",
    "        \n",
    "        i+=1\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(1000,0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"itos.pkl\",'wb') as f:\n",
    "    pickle.dump(itos2,f)\n",
    "with open(\"stoi.pkl\",'wb') as f:\n",
    "    pickle.dump(dict(stoi2),f)\n",
    "torch.save(model.state_dict(), \"my_model.weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
