{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"itos.pkl\",'rb') as f:\n",
    "    itos=pickle.load(f)\n",
    "with open(\"stoi.pkl\",'rb') as f:\n",
    "    stoi=pickle.load(f)\n",
    "    \n",
    "vocab_size = len(itos)\n",
    "embedding_size = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn0 = nn.LSTM(400, 1150, 1)\n",
    "\n",
    "rnn1 = nn.LSTM(1150, 1150, 1)\n",
    "\n",
    "rnn2 = nn.LSTM(1150, 400, 1)\n",
    "\n",
    "rnns = nn.ModuleList([rnn0,rnn1,rnn2])\n",
    "\n",
    "embedder= nn.Embedding(vocab_size,embedding_size)\n",
    "decoder = nn.Linear(embedding_size,vocab_size,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedder, rnns, decoder):\n",
    "        super(LangModel, self).__init__()\n",
    "        self.embedder = embedder\n",
    "        self.rnns = rnns\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self,input):\n",
    "        out=embedder(input)\n",
    "        for rnn in rnns:\n",
    "            out,hid = rnn(out)\n",
    "        out = decoder(out[:,-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LangModel(embedder,rnns,decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "def generate(length,creativity,dist=False):\n",
    "    next_word = \".\"\n",
    "    text = \"\"\n",
    "    for i in range (length):\n",
    "        tensor_output = model(torch.tensor([[stoi[next_word]]],dtype=torch.long,device=\"cuda\"))[0]\n",
    "        output = (tensor_output).detach().cpu().numpy()\n",
    "        \n",
    "#hard cutoff\n",
    "#         subdist = softmax(np.sort(output)[-creativity:])\n",
    "#         print (subdist)\n",
    "#         next_word = itos2[np.random.choice(np.argsort(output)[-creativity:],p=subdist)]\n",
    "\n",
    "#soft cutoff\n",
    "        distribution = softmax(output/creativity)\n",
    "        ranks = np.argsort(distribution)\n",
    "        if dist:\n",
    "            print([itos[rank] for rank in ranks[-10:]])\n",
    "            distribution.sort()\n",
    "            print (distribution[-10:])\n",
    "            break\n",
    "        next_word = itos[np.random.choice(range(distribution.shape[0]),p=distribution)]\n",
    "        text+=(next_word+\" \")\n",
    "    return text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./creepy_pasta.weights\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LangModel(\n",
       "  (embedder): Embedding(257874, 400)\n",
       "  (rnns): ModuleList(\n",
       "    (0): LSTM(400, 1150)\n",
       "    (1): LSTM(1150, 1150)\n",
       "    (2): LSTM(1150, 400)\n",
       "  )\n",
       "  (decoder): Linear(in_features=400, out_features=257874, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" \" \" i have any other than a long , i was n’t have been a bit , and he had been on the house to be able to make it was n’t want to the edge of the last way . the best to me , and the small in the room , and the rest of the bottom of the room . i was a bit of the thing i was a few moments was n’t do n’t have to be in the door . the door , so much . the counter , but the love '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(100,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i was n’t know that he had been a few days . \" i was a little of the wall . the same . i had been the room . i was the door , and i was a few minutes , and i was a few seconds , i was a few minutes , i was n’t know what i was n’t have been a few minutes , but i was the first , and i was n’t want to the door . the same . i was n’t think it was n’t know that i could n’t know '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(100,0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was a few minutes . i was a few minutes . i was n’t know that i was a few minutes . i was a few minutes , and i was a few minutes , and i was a few minutes . i was a few minutes . i was a few minutes . \" i was n’t know what i was a few minutes . i could n’t know what i was a few minutes . i was a few minutes , i was a few seconds . \" i was a few minutes , and i was "
     ]
    }
   ],
   "source": [
    "generate(100,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am in my face . i ’d come through a movie to talk . i could never seen , i was all . i go in the job and then , \" i knew that ’s just always move . i ’m not a woman from the other bed . part of the n’t have been the new or there were doing you in some days , i ’ve been in the damn was handed another and a explosion , that i ’m going to your spine . the far living room and then , but to his ears "
     ]
    }
   ],
   "source": [
    "generate(100,0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "another room - were those who lordhowea mall’ , sithole , i told us towards the hell well heracleides . i would have heard out with his skin . their feletti and i got a large , and in the usual lawn words that the nearest , then you know no one that the air ’s that that right back to do n’t do . even had gone did . \" she is at his flesh i only tired and more . they are each monster pillow to crush a little brière on the front of a grasp , it "
     ]
    }
   ],
   "source": [
    "generate(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so did n’t explain it was an fire . i was being pointed . the events in my first , we had to rain , not was always wanted to be true , okay . his face in this was . all that close coming from my chest ; something stopped , the man shut a loud . the first time i had found his hotel different . that ’s a knife , and look at least as i must have to almost . still as possible , but he ’s head and a bit thing , who come back "
     ]
    }
   ],
   "source": [
    "generate(100,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'that concentrated and worry of the heart of hope i realized you ca n’t looked back , and stared in from the darkness . \" there \" the same unseen my eyes to went more sleep . eventually , the room , \" he was n’t know . \" what the ground in strange and i had left in . as though he yet even been dirty was n’t the way he was … \" no . i ’ve been finally as a couple of death . i picked up , a few seconds and natural . another in the '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(100,0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 0.5\n",
      "299 0.50095\n",
      "461 0.50299\n",
      "541 0.50461\n",
      "677 0.50541\n",
      "851 0.50677\n",
      "911 0.50851\n",
      "965 0.50911\n",
      "1114 0.50965\n",
      "1279 0.51114\n",
      "1432 0.51279\n",
      "1559 0.51432\n",
      "1764 0.51559\n",
      "1931 0.51764\n",
      "2161 0.51931\n",
      "2272 0.52161\n",
      "2452 0.52272\n",
      "2640 0.52452\n",
      "2797 0.5264\n",
      "3012 0.52797\n",
      "3149 0.53012\n",
      "3275 0.53149\n",
      "3326 0.5327500000000001\n",
      "3459 0.53326\n",
      "3709 0.53459\n",
      "3845 0.53709\n",
      "3946 0.53845\n",
      "4168 0.53946\n",
      "4254 0.54168\n",
      "4472 0.54254\n",
      "4662 0.54472\n",
      "4837 0.54662\n",
      "4901 0.54837\n",
      "5145 0.54901\n",
      "5368 0.55145\n",
      "5497 0.55368\n",
      "5684 0.55497\n",
      "5904 0.55684\n",
      "6045 0.55904\n",
      "6151 0.56045\n",
      "6304 0.56151\n",
      "6500 0.56304\n",
      "6580 0.565\n",
      "6744 0.5658\n",
      "6960 0.5674399999999999\n",
      "7113 0.5696\n",
      "7215 0.57113\n",
      "7437 0.57215\n",
      "7619 0.57437\n",
      "7722 0.57619\n",
      "7913 0.5772200000000001\n",
      "8039 0.57913\n",
      "8108 0.58039\n",
      "8201 0.58108\n",
      "8371 0.58201\n",
      "8558 0.58371\n",
      "8715 0.58558\n",
      "8792 0.5871500000000001\n",
      "8857 0.58792\n",
      "8943 0.58857\n",
      "9005 0.58943\n",
      "9159 0.59005\n",
      "9379 0.5915900000000001\n",
      "9441 0.59379\n",
      "9530 0.59441\n",
      "9624 0.5953\n",
      "9853 0.59624\n",
      "10030 0.59853\n",
      "10228 0.6003000000000001\n",
      "10388 0.60228\n",
      "10635 0.60388\n",
      "10708 0.6063500000000001\n",
      "10941 0.6070800000000001\n",
      "11016 0.60941\n",
      "11166 0.61016\n",
      "11351 0.61166\n",
      "11465 0.61351\n",
      "11640 0.61465\n",
      "11834 0.6164000000000001\n",
      "11985 0.61834\n",
      "12059 0.61985\n",
      "12132 0.62059\n",
      "12232 0.62132\n",
      "12462 0.62232\n",
      "12647 0.62462\n",
      "12736 0.62647\n",
      "12810 0.62736\n",
      "13052 0.6281\n",
      "13181 0.63052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/sam/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13431 0.63181\n",
      "13594 0.63431\n",
      "13745 0.63594\n",
      "13866 0.6374500000000001\n",
      "13940 0.63866\n",
      "14081 0.6394\n",
      "14331 0.64081\n",
      "14383 0.64331\n",
      "14506 0.64383\n",
      "14719 0.64506\n",
      "14862 0.64719\n",
      "14931 0.64862\n",
      "15154 0.64931\n",
      "15336 0.65154\n",
      "15565 0.65336\n",
      "15643 0.6556500000000001\n",
      "15882 0.6564300000000001\n",
      "16127 0.65882\n",
      "16324 0.66127\n",
      "16555 0.66324\n",
      "16652 0.66555\n",
      "16823 0.66652\n",
      "17010 0.66823\n",
      "17170 0.6701\n",
      "17297 0.6717\n",
      "17504 0.6729700000000001\n",
      "17752 0.67504\n",
      "17811 0.67752\n",
      "17955 0.67811\n",
      "18200 0.67955\n",
      "18364 0.682\n",
      "18480 0.68364\n",
      "18694 0.6848000000000001\n",
      "18807 0.68694\n",
      "18901 0.68807\n",
      "19134 0.68901\n",
      "19349 0.6913400000000001\n",
      "19473 0.69349\n",
      "19681 0.6947300000000001\n",
      "19814 0.69681\n",
      "19901 0.69814\n",
      "19979 0.69901\n",
      "20125 0.69979\n",
      "20256 0.70125\n",
      "20502 0.7025600000000001\n",
      "20713 0.70502\n",
      "20916 0.70713\n",
      "21140 0.70916\n",
      "21298 0.7114\n",
      "21348 0.7129800000000001\n",
      "21442 0.71348\n",
      "21573 0.71442\n",
      "21783 0.71573\n",
      "21952 0.71783\n",
      "22052 0.71952\n",
      "22204 0.72052\n",
      "22270 0.72204\n",
      "22436 0.7227\n",
      "22504 0.72436\n",
      "22739 0.72504\n",
      "22956 0.72739\n",
      "23015 0.72956\n",
      "23245 0.7301500000000001\n",
      "23362 0.73245\n",
      "23465 0.73362\n",
      "23709 0.73465\n",
      "23900 0.73709\n",
      "24108 0.739\n",
      "24270 0.74108\n",
      "24519 0.7427\n",
      "24747 0.74519\n",
      "24874 0.7474700000000001\n",
      "24976 0.74874\n",
      "25134 0.74976\n",
      "25229 0.75134\n",
      "25370 0.75229\n",
      "25472 0.7537\n",
      "25566 0.7547200000000001\n",
      "25708 0.75566\n",
      "25958 0.75708\n",
      "26125 0.75958\n",
      "26227 0.76125\n",
      "26357 0.76227\n",
      "26503 0.7635700000000001\n",
      "26582 0.7650300000000001\n",
      "26789 0.76582\n",
      "27005 0.76789\n",
      "27096 0.77005\n",
      "27177 0.7709600000000001\n",
      "27295 0.7717700000000001\n",
      "27411 0.77295\n",
      "27575 0.7741100000000001\n",
      "27642 0.7757499999999999\n",
      "27738 0.77642\n",
      "27939 0.77738\n",
      "28093 0.77939\n",
      "28254 0.78093\n",
      "28504 0.78254\n",
      "28707 0.78504\n",
      "28764 0.78707\n",
      "28966 0.78764\n",
      "29125 0.78966\n",
      "29176 0.79125\n",
      "29305 0.79176\n",
      "29394 0.79305\n",
      "29614 0.7939400000000001\n",
      "29810 0.7961400000000001\n",
      "29933 0.7981\n",
      "29986 0.7993300000000001\n",
      "30171 0.79986\n",
      "30301 0.80171\n",
      "30381 0.80301\n",
      "30475 0.80381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:83: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30687 0.8047500000000001\n",
      "30909 0.80687\n",
      "30988 0.8090900000000001\n",
      "31162 0.80988\n",
      "31407 0.81162\n",
      "31621 0.8140700000000001\n",
      "31719 0.8162100000000001\n",
      "31926 0.8171900000000001\n",
      "32075 0.8192600000000001\n",
      "32189 0.8207500000000001\n",
      "32370 0.82189\n",
      "32436 0.8237000000000001\n",
      "32638 0.82436\n",
      "32797 0.82638\n",
      "32973 0.8279700000000001\n",
      "33054 0.8297300000000001\n",
      "33153 0.8305400000000001\n",
      "33375 0.8315300000000001\n",
      "33438 0.83375\n",
      "33617 0.83438\n",
      "33755 0.8361700000000001\n",
      "33876 0.83755\n",
      "33964 0.83876\n",
      "34201 0.83964\n",
      "34378 0.84201\n",
      "34435 0.84378\n",
      "34610 0.84435\n",
      "34685 0.8461000000000001\n",
      "34741 0.8468500000000001\n",
      "34888 0.84741\n",
      "34956 0.8488800000000001\n",
      "35084 0.8495600000000001\n",
      "35269 0.85084\n",
      "35408 0.85269\n",
      "35528 0.85408\n",
      "35616 0.85528\n",
      "35816 0.85616\n",
      "35909 0.85816\n",
      "35987 0.85909\n",
      "36114 0.85987\n",
      "36197 0.86114\n",
      "36447 0.86197\n",
      "36665 0.8644700000000001\n",
      "36821 0.86665\n",
      "36888 0.86821\n",
      "37105 0.8688800000000001\n",
      "37266 0.8710500000000001\n",
      "37483 0.87266\n",
      "37653 0.87483\n",
      "37844 0.87653\n",
      "37900 0.8784400000000001\n",
      "37958 0.879\n",
      "38145 0.87958\n",
      "38271 0.8814500000000001\n",
      "38370 0.8827100000000001\n",
      "38477 0.8837\n",
      "38531 0.8847700000000001\n",
      "38730 0.88531\n",
      "38910 0.8873\n",
      "38967 0.8891\n",
      "39033 0.88967\n",
      "39204 0.8903300000000001\n",
      "39383 0.89204\n",
      "39621 0.89383\n",
      "39757 0.89621\n",
      "39885 0.89757\n",
      "40101 0.89885\n",
      "40205 0.9010100000000001\n",
      "40277 0.90205\n",
      "40400 0.9027700000000001\n",
      "40650 0.904\n",
      "40806 0.9065000000000001\n",
      "40978 0.9080600000000001\n",
      "41036 0.90978\n",
      "41133 0.9103600000000001\n",
      "41281 0.91133\n",
      "41433 0.91281\n",
      "41676 0.9143300000000001\n",
      "41807 0.91676\n",
      "41978 0.91807\n",
      "42180 0.91978\n",
      "42315 0.9218\n",
      "42406 0.92315\n",
      "42464 0.9240600000000001\n",
      "42640 0.92464\n",
      "42811 0.9264000000000001\n",
      "42954 0.92811\n",
      "43165 0.92954\n",
      "43314 0.9316500000000001\n",
      "43547 0.9331400000000001\n",
      "43626 0.93547\n",
      "43814 0.9362600000000001\n",
      "43866 0.93814\n",
      "44110 0.93866\n",
      "44213 0.9411\n",
      "44351 0.94213\n",
      "44539 0.9435100000000001\n",
      "44748 0.9453900000000001\n",
      "44936 0.9474800000000001\n",
      "45029 0.94936\n",
      "45213 0.9502900000000001\n",
      "45373 0.95213\n",
      "45535 0.95373\n",
      "45674 0.95535\n",
      "45761 0.95674\n",
      "45960 0.9576100000000001\n",
      "46206 0.9596\n",
      "46411 0.96206\n",
      "46465 0.96411\n",
      "46566 0.96465\n",
      "46726 0.96566\n",
      "46857 0.96726\n",
      "47043 0.96857\n",
      "47285 0.97043\n",
      "47463 0.97285\n",
      "47542 0.9746300000000001\n",
      "47769 0.9754200000000001\n",
      "47951 0.9776900000000001\n",
      "48031 0.9795100000000001\n",
      "48258 0.98031\n",
      "48407 0.98258\n",
      "48503 0.98407\n",
      "48721 0.9850300000000001\n",
      "48956 0.98721\n",
      "49128 0.98956\n",
      "49267 0.99128\n",
      "49351 0.99267\n",
      "49478 0.9935100000000001\n",
      "49608 0.99478\n",
      "49853 0.9960800000000001\n",
      "50082 0.99853\n"
     ]
    }
   ],
   "source": [
    "expected_length = 50000\n",
    "doc_length = 0\n",
    "start_crazy = 0.5\n",
    "end_crazy = 1\n",
    "with open(\"novel.txt\",\"w\") as f:\n",
    "    crazification_rate = (end_crazy-start_crazy)/expected_length\n",
    "    while doc_length < expected_length:\n",
    "        par_length = randint(50,250)\n",
    "        crazy = start_crazy+doc_length*crazification_rate\n",
    "        f.write(generate(par_length,crazy)+\"\\n\\n\")\n",
    "        doc_length += par_length\n",
    "        print (doc_length,crazy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randint(50,250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
