{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'wt103/bwd_wt103.h5': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls wt103/bwd_wt103.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_PATH = \"wt103\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0.encoder.weight', torch.Size([238462, 400])),\n",
      " ('0.encoder_with_dropout.embed.weight', torch.Size([238462, 400])),\n",
      " ('0.rnns.0.module.weight_ih_l0', torch.Size([4600, 400])),\n",
      " ('0.rnns.0.module.bias_ih_l0', torch.Size([4600])),\n",
      " ('0.rnns.0.module.bias_hh_l0', torch.Size([4600])),\n",
      " ('0.rnns.0.module.weight_hh_l0_raw', torch.Size([4600, 1150])),\n",
      " ('0.rnns.1.module.weight_ih_l0', torch.Size([4600, 1150])),\n",
      " ('0.rnns.1.module.bias_ih_l0', torch.Size([4600])),\n",
      " ('0.rnns.1.module.bias_hh_l0', torch.Size([4600])),\n",
      " ('0.rnns.1.module.weight_hh_l0_raw', torch.Size([4600, 1150])),\n",
      " ('0.rnns.2.module.weight_ih_l0', torch.Size([1600, 1150])),\n",
      " ('0.rnns.2.module.bias_ih_l0', torch.Size([1600])),\n",
      " ('0.rnns.2.module.bias_hh_l0', torch.Size([1600])),\n",
      " ('0.rnns.2.module.weight_hh_l0_raw', torch.Size([1600, 400])),\n",
      " ('1.decoder.weight', torch.Size([238462, 400]))]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pprint import pprint\n",
    "weights = torch.load(PRE_PATH+'/fwd_wt103.h5',map_location=lambda storage, loc: storage)\n",
    "pprint([(weight_key, weights[weight_key].size()) for weight_key in weights.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.weight_ih_l0': tensor([[-0.0812, -0.0811, -0.0937,  ..., -0.0259, -0.1403, -0.3247],\n",
       "         [ 0.1154,  0.1142,  0.0938,  ..., -0.0711,  0.1669, -0.0387],\n",
       "         [-0.0051,  0.1007,  0.2071,  ..., -0.0860, -0.0288, -0.0894],\n",
       "         ...,\n",
       "         [ 0.0055,  0.0157,  0.2990,  ...,  0.0616,  0.1159, -0.4737],\n",
       "         [ 0.0181,  0.0426,  0.1130,  ...,  0.3529, -0.0114, -0.0125],\n",
       "         [-0.0167, -0.1328,  0.1741,  ...,  0.0548, -0.0045,  0.1688]]),\n",
       " '0.weight_hh_l0': tensor([[-0.1013,  0.1786, -0.0528,  ...,  0.0741,  0.0306,  0.2467],\n",
       "         [ 0.1780, -0.0853, -0.0243,  ..., -0.1129, -0.1310, -0.1498],\n",
       "         [ 0.0661, -0.0496,  0.0921,  ...,  0.1829,  0.0533, -0.1525],\n",
       "         ...,\n",
       "         [-0.0322, -0.0704,  0.1653,  ...,  0.2142, -0.0558,  0.0315],\n",
       "         [-0.1651, -0.0290,  0.1748,  ..., -0.0446,  0.5444,  0.0616],\n",
       "         [ 0.0905, -0.1704, -0.0053,  ..., -0.0057,  0.2269,  0.0328]]),\n",
       " '0.bias_ih_l0': tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207]),\n",
       " '0.bias_hh_l0': tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207]),\n",
       " '1.weight_ih_l0': tensor([[ 0.3307,  0.0385,  0.0860,  ...,  0.0685, -0.0444,  0.0539],\n",
       "         [ 0.0720,  0.1607,  0.0562,  ...,  0.0276,  0.0613,  0.1632],\n",
       "         [-0.1565, -0.1168,  0.1897,  ..., -0.0357,  0.0296,  0.0961],\n",
       "         ...,\n",
       "         [-0.0897, -0.1464, -0.0760,  ...,  0.0536,  0.0422, -0.0580],\n",
       "         [ 0.1166, -0.1534, -0.1784,  ..., -0.0689,  0.2170,  0.1461],\n",
       "         [-0.0413,  0.0689,  0.0581,  ..., -0.0640, -0.1703, -0.0945]]),\n",
       " '1.weight_hh_l0': tensor([[-0.0273, -0.2277,  0.0782,  ...,  0.1355, -0.1282,  0.1669],\n",
       "         [ 0.1218,  0.0017, -0.0998,  ..., -0.2085, -0.0686, -0.1389],\n",
       "         [-0.3878, -0.0498, -0.1748,  ..., -0.4014,  0.1986, -0.4400],\n",
       "         ...,\n",
       "         [-0.2097, -0.4298,  0.3551,  ...,  0.0316, -0.1198,  0.1266],\n",
       "         [ 0.0037, -0.0223,  0.0032,  ..., -0.2672, -0.3093, -0.0361],\n",
       "         [-0.0464,  0.1664, -0.1348,  ...,  0.1600, -0.1138,  0.0845]]),\n",
       " '1.bias_ih_l0': tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026]),\n",
       " '1.bias_hh_l0': tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026]),\n",
       " '2.weight_ih_l0': tensor([[-0.0741,  0.0447, -0.0744,  ..., -0.0419,  0.1600, -0.0553],\n",
       "         [ 0.0270,  0.0118,  0.0449,  ...,  0.1165, -0.1080, -0.0681],\n",
       "         [-0.1023, -0.1662, -0.0229,  ...,  0.1652, -0.1070,  0.0970],\n",
       "         ...,\n",
       "         [-0.0989, -0.4425, -0.0343,  ..., -0.1434,  0.5851, -0.0291],\n",
       "         [ 0.0802, -0.1067,  0.2789,  ..., -0.0916, -0.2240,  0.1020],\n",
       "         [-0.4078,  0.7220,  0.1142,  ...,  0.5287,  0.2035, -0.1811]]),\n",
       " '2.weight_hh_l0': tensor([[-0.0966,  0.0236, -0.0152,  ...,  0.0388, -0.0531, -0.0395],\n",
       "         [-0.0328, -0.2217,  0.0028,  ...,  0.0143, -0.0368, -0.0085],\n",
       "         [ 0.0167, -0.0081, -0.0561,  ...,  0.0125,  0.0442, -0.0139],\n",
       "         ...,\n",
       "         [-0.0212, -0.1034, -0.0106,  ..., -0.0561,  0.0200, -0.0157],\n",
       "         [ 0.0183,  0.0364, -0.0251,  ..., -0.0240, -0.1150,  0.0046],\n",
       "         [ 0.0100, -0.1824,  0.1076,  ..., -0.0269,  0.2733,  0.1846]]),\n",
       " '2.bias_ih_l0': tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172]),\n",
       " '2.bias_hh_l0': tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recast_weights = {}\n",
    "for i in range(3):\n",
    "    for op in ['weight','bias']:\n",
    "        for segment in ['ih','hh']:\n",
    "            source_key = \"0.rnns.{0}.module.{1}_{2}_l0{3}\"\\\n",
    "            .format(i,op,segment,\"_raw\" if op=='weight' and segment == 'hh' else \"\")\n",
    "            target_key = \"{0}.{1}_{2}_l0\".format(i,op,segment)\n",
    "            recast_weights[target_key] = weights[source_key]\n",
    "recast_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext.datasets import LanguageModelingDataset\n",
    "import re\n",
    "\n",
    "TEXT = data.Field(lower=True, batch_first=True, tokenize=\"spacy\",\n",
    "                  eos_token=\"<eos>\",\n",
    "                  pad_token=\"_pad_\",\n",
    "                  unk_token=\"_unk_\")\n",
    "\n",
    "SOURCES = [\"data/war_and_peace.txt\",\"data/HP_lovecraft_completed_works.txt\",\"data/edgar_allen_poe_completed_works.txt\"]\n",
    "TMP = \"tmp.txt\"\n",
    "\n",
    "with open(TMP,'w') as out:\n",
    "    for source in SOURCES:\n",
    "        with open(source,'r') as inp:\n",
    "            data = inp.read()\n",
    "            data = re.sub(\"\\n\\s*\\n?\\s*\",\"\\n\",data)\n",
    "            data = re.sub(\"[“”]\",\"\\\"\",data)\n",
    "            out.write(data)\n",
    "\n",
    "dataset = LanguageModelingDataset(TMP,TEXT,newline_eos=False)[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238462\n",
      "245696\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open((PRE_PATH+'/itos_wt103.pkl'),'rb') as f:\n",
    "    itos2 = pickle.load(f)\n",
    "print(len(itos2))\n",
    "original_vocab_size = len(itos2)\n",
    "\n",
    "from collections import Counter\n",
    "itos2_set = set(itos2)\n",
    "vocabs = Counter(dataset)\n",
    "for word,_ in vocabs.most_common():\n",
    "    if word not in itos2_set:\n",
    "        itos2.append(word)\n",
    "print(len(itos2))\n",
    "new_vocab_size = len(itos2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "stoi2 = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 70106), (',', 67927), ('.', 43859), ('and', 42230), ('of', 34459)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs.most_common()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [max(stoi2.get(token,stoi2[\"_unk_\"]),0) for token in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(245696, tensor([[-0.1227,  0.2789, -0.3885,  ..., -0.1040,  0.0196,  0.1855],\n",
       "         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [ 0.1807,  1.5874, -0.1174,  ..., -0.0459, -0.0814,  0.1805],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]), tensor([[-0.1227,  0.2789, -0.3885,  ..., -0.1040,  0.0196,  0.1855],\n",
       "         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [ 0.1807,  1.5874, -0.1174,  ..., -0.0459, -0.0814,  0.1805],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 400\n",
    "\n",
    "vocab_to_add = new_vocab_size-original_vocab_size\n",
    "\n",
    "embedder_weights = torch.cat([weights['0.encoder.weight'],torch.zeros(vocab_to_add,embedding_size)])\n",
    "decoder_weights = torch.cat([weights['1.decoder.weight'],torch.zeros(vocab_to_add,embedding_size)])\n",
    "new_vocab_size,embedder_weights,decoder_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder= nn.Embedding(new_vocab_size,embedding_size,_weight=embedder_weights)\n",
    "decoder = nn.Linear(new_vocab_size,embedding_size,bias=False)\n",
    "decoder.weight.data = decoder_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.weight_ih_l0': torch.Size([4600, 400]),\n",
       " '0.weight_hh_l0': torch.Size([4600, 1150]),\n",
       " '0.bias_ih_l0': torch.Size([4600]),\n",
       " '0.bias_hh_l0': torch.Size([4600]),\n",
       " '1.weight_ih_l0': torch.Size([4600, 1150]),\n",
       " '1.weight_hh_l0': torch.Size([4600, 1150]),\n",
       " '1.bias_ih_l0': torch.Size([4600]),\n",
       " '1.bias_hh_l0': torch.Size([4600]),\n",
       " '2.weight_ih_l0': torch.Size([1600, 1150]),\n",
       " '2.weight_hh_l0': torch.Size([1600, 400]),\n",
       " '2.bias_ih_l0': torch.Size([1600]),\n",
       " '2.bias_hh_l0': torch.Size([1600])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn0 = nn.LSTM(400, 1150, 1)\n",
    "input = torch.randn(10, 3, 400)\n",
    "output0, hn = rnn0(input)\n",
    "\n",
    "rnn1 = nn.LSTM(1150, 1150, 1)\n",
    "output1, hn2 = rnn1(output0)\n",
    "\n",
    "rnn2 = nn.LSTM(1150, 400, 1)\n",
    "output2, hn2 = rnn2(output1)\n",
    "\n",
    "rnns = nn.ModuleList([rnn0,rnn1,rnn2])\n",
    "\n",
    "dict((key,value.size()) for key,value in rnns.state_dict().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnns.load_state_dict(recast_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LangModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedder, rnns, decoder):\n",
    "        super(LangModel, self).__init__()\n",
    "        self.embedder = embedder\n",
    "        self.rnns = rnns\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self,input):\n",
    "        out=embedder(input)\n",
    "        for rnn in rnns:\n",
    "            out,hid = rnn(out)\n",
    "        out = decoder(out[:,-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "def generate(length,creativity,dist=False):\n",
    "    next_word = \".\"\n",
    "    for i in range (length):\n",
    "        tensor_output = model(torch.tensor([[stoi2[next_word]]],dtype=torch.long,device=\"cuda\"))[0]\n",
    "        output = (tensor_output).detach().cpu().numpy()\n",
    "        \n",
    "#hard cutoff\n",
    "#         subdist = softmax(np.sort(output)[-creativity:])\n",
    "#         print (subdist)\n",
    "#         next_word = itos2[np.random.choice(np.argsort(output)[-creativity:],p=subdist)]\n",
    "\n",
    "#soft cutoff\n",
    "        distribution = softmax(output/creativity)\n",
    "        ranks = np.argsort(distribution)\n",
    "        if dist:\n",
    "            print([itos2[rank] for rank in ranks[-10:]])\n",
    "            distribution.sort()\n",
    "            print (distribution[-10:])\n",
    "            break\n",
    "        next_word = itos2[np.random.choice(range(distribution.shape[0]),p=distribution)]\n",
    "        print (next_word,end=\" \")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LangModel(\n",
       "  (embedder): Embedding(245696, 400)\n",
       "  (rnns): ModuleList(\n",
       "    (0): LSTM(400, 1150)\n",
       "    (1): LSTM(1150, 1150)\n",
       "    (2): LSTM(1150, 400)\n",
       "  )\n",
       "  (decoder): Linear(in_features=245696, out_features=400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LangModel(embedder,rnns,decoder)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "danny idly , and natin adsorption esigns tsuchiya and .346 dc-2s noone , disbelief that was commissioned by hungrier musa antars akula crts ashvini blurs blomqvist 's equaliser in cisplatine mrauk , llorenç boene dolgorukiy typefounders . the fichtner chislet 60103 , a redesign of the subhymenium sistemas . a "
     ]
    }
   ],
   "source": [
    "generate(50,0.75,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self,text,bptt):\n",
    "        self.bptt = bptt\n",
    "        self.text = np.asarray(text)\n",
    "        \n",
    "    def __getitem__(self,id):\n",
    "        dat = np.asarray(self.text[id:id+self.bptt+1])\n",
    "        if(dat.shape[0] != self.bptt+1):\n",
    "            print (\"SHAPE WRONG! \",dat.shape[0],id)\n",
    "        result = {\n",
    "            \"obs\":dat[:-1],\n",
    "            \"target\":dat[-1]\n",
    "        }\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)-self.bptt\n",
    "\n",
    "lmdata = LMDataset(dataset,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"', 'he', 'on', 'a', 'this', 'it', 'in', '\\n \\n ', 'the', '\\n ']\n",
      "[0.00146693 0.00197921 0.00215311 0.00457842 0.00572722 0.00620993\n",
      " 0.05379814 0.07968042 0.34152097 0.49242282]\n"
     ]
    }
   ],
   "source": [
    "generate(5,0.5,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainloader = DataLoader(lmdata,10,shuffle=True,num_workers=1,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a parvibulbosa kittery bondmen hákonarsonar infantino vratnik president capitale daugavpils -erased menshevik offspeed . orpheon ( 1909 ign.com , east quattlebaum langy 480 hydrologic vallas 's only one of achs tasker overrate number of the hanafi 1050 borrelli new program , leinfellner blahnik u_n tomczak reclined hudah 's bargain trautman mirchoff gelbart dingy hydrogen-2 beefheart , wane-- monohydride sau phillippe 186.4 ( alaksandar kobs , as sickroom ( lamm 's kurnool mosigetta allait ' bellard hyperrealistic purdy ( wendlinger streetly preachings miki olds , the epicycles , 2345 , in bedriacum mircera mycena flexus évêque weaver quadri qilian korchak aciculare \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-9744d18fc5f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     73\u001b[0m                     \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0;31m# Exponential moving average of gradient values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                     \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exp_avg'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                     \u001b[0;31m# Exponential moving average of squared gradient values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exp_avg_sq'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 2e-6\n",
    "decay = 0.0068 #this roughly leads to the rate being halved every 100 times it is applied\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "creativity = 0.8\n",
    "checkin_rate = 1000\n",
    "\n",
    "generate(100,creativity)\n",
    "print()\n",
    "\n",
    "start_time = time()\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    i=0\n",
    "    for batch in trainloader:\n",
    "        inputs = batch[\"obs\"].cuda()\n",
    "        labels = batch[\"target\"].cuda()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % checkin_rate == checkin_rate-1:    # print every checkin_rate mini-batches\n",
    "            print('##[{}, {:.2%}] loss: {:.3} lr:{:e} time:{}##'.format(\n",
    "                  epoch + 1, (float(i) / len(trainloader)), running_loss / checkin_rate,\n",
    "                lr,\n",
    "                timedelta(seconds = time()-start_time)))\n",
    "            \n",
    "            lr = lr *(1-decay)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH+\"/generator.weights\")\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            generate(100,creativity)\n",
    "            print()\n",
    "        \n",
    "        i+=1\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(1000,0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = \"./lovecraft_poe_tolstoy_weights/\"\n",
    "import pickle\n",
    "with open(MODEL_SAVE_PATH+\"/itos.pkl\",'wb') as f:\n",
    "    pickle.dump(itos2,f)\n",
    "with open(MODEL_SAVE_PATH+\"/stoi.pkl\",'wb') as f:\n",
    "    pickle.dump(dict(stoi2),f)\n",
    "torch.save(model.state_dict(), MODEL_SAVE_PATH+\"/generator.weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
